# -*- coding: utf-8 -*-
#import scrapy
from scrapy import Spider
from scrapy.http import Request

#import scrapy items
from exploit_crawlers.items import ExploitCrawlersItem

#Use sleep function to throttle ussage
from time import sleep
import random


class PacketstormSpider(Spider):
    name = 'packetstorm'
    allowed_domains = ['packetstormsecurity.com']
    start_urls = ['https://packetstormsecurity.com/files/tags/exploit/']

    def parse(self, response):
        exploits = response.xpath('//*[@class="ico text-plain"]/@href').extract()
        for exploit in exploits:
            absolute_url = response.urljoin(exploit)
            yield Request(absolute_url, callback=self.parse_exploit)

    def parse_exploit(self, response):
        #extract data
        title = response.xpath('//*[@class="ico text-plain"]/strong/text()').extract_first()
        date = response.xpath('//*[@class="datetime"]/a/text()').extract_first()
        credit = response.xpath('//*[@class="refer"]/a/text()').extract_first()
        description = response.xpath('//*[@class="detail"]/p/text()').extract_first()
        code = response.xpath('//*[@class="src"]/pre/code/text()').extract()

        #Process Item
        item = ExploitCrawlersItem()

        item['title'] = title
        item['date'] = date
        item['credit'] = credit
        item['description'] = description
        item['code'] = code

        return item
